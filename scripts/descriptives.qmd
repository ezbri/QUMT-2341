---
title: "descriptives"
format: 
  html: 
    self-contained: true
    toc: true             
  pdf: 
    number-sections: true 
    toc: true            
    pdf-engine: xelatex 
editor: visual
---

```{r}
#| include: false
library(psych)
library(ggplot2)
library(dplyr)
library(lsr)
```

```{r}
starwars <- dplyr::starwars
```

```{r}
names(starwars)
```

```{r}
glimpse(starwars)
```

# Descriptive Statistics

| [P]{.underline}opulation | [S]{.underline}ample     |
|--------------------------|--------------------------|
| [P]{.underline}arameters | [S]{.underline}tatistics |

## Location / Central Tendency

### Mode

The mode of a sample is very simple: it is the value that occurs **most frequently**. 

A frequency table could be examined.

```{r}
freq(as.ordered(starwars$eye_color), plot = FALSE)
```

```{r}
modeOf(x = starwars$eye_color)
```

#### No Mode = same frequency

```{r}
y = 100

ggplot() +
  geom_hline(yintercept = y)
```

#### Bimodal = two modes

```{r}
# Create data
set.seed(123)

n <- 1000 # Sample split in each below

x <- c(
  rnorm(n / 2, mean = 0, sd = 1), # Update mean or sd
  rnorm(n / 2, mean = 3, sd = 1)
)

df <- data.frame(x)

# Visualize
ggplot(df, aes(x)) +
  geom_histogram(bins = 40)
```

#### Multimodal = \>2 modes

```{r}
# Create data
set.seed(123)

n <- 1500 # Sample split in each below

x <- c(
  rnorm(n / 3, mean = 0, sd = 1), # Update mean or sd
  rnorm(n / 3, mean = 3, sd = 1),
  rnorm(n / 3, mean = 6, sd = 1)
)

df <- data.frame(x)

# Visualize
ggplot(df, aes(x)) +
  geom_histogram(bins = 40)
```

### Median

The median of a set of observations is just the **middle value**. Let’s imagine we were interested only in the first 5 AFL winning margins. To figure out the median, we sort these numbers into ascending order:

$$
8, 31,\mathbf{32},56,56 
$$

From inspection, it’s obvious that the median value of these 5 observations is 32, since that’s the middle one in the sorted list (I’ve put it in bold to make it even more obvious). Easy stuff.

But what should we do if we were interested in the first 6 games rather than the first 5? Since the sixth game in the season had a winning margin of 14 points, our sorted list is now.

$$
8,14,\mathbf{31,32},56,56
$$

and there are *two* middle numbers, 31 and 32. The median is defined as the average of those two numbers, which is of course 31.5. 

As before, it’s very tedious to do this by hand when you’ve got lots of numbers. In real life, of course, no-one actually calculates the median by sorting the data and then looking for the middle value. In real life, we use the median command:

```{r}
median(starwars$mass, na.rm = TRUE)
# If you variable contains even one NA, this function will return NA, unless you tell it to ignore the NAs. 
```

### Arithmetic Mean

*Typical, old-fashioned average. Add all of the values up, and then divide by the total number of values.*

$$
\frac{1}{n}\left(x_1 + x_2 + \cdots + x_n\right)
$$

where $x_i, x_2 \ldots x_n$ are the observed values and $n$ is the number of observations.

| Population | Sample |
|----|----|
| $$                     
 \mu = \frac{\sum y}{N}  
 $$ | $$                   
                          m = \frac{\sum y}{n}  
                          $$ |

For example, for the values 56, 31, 56, 8, 32:

$$
\bar{x} = \frac{x_1 + x_2 + x_3 + \cdots + x_n}{n}
$$

$$
\bar{x} = \frac{56 + 31 + 56 + 8 + 32}{5}
$$

$$
\bar{x} = \frac{183}{5} = 36.60
$$

Written out:

```{r}
(56 + 31 + 56 + 8 + 32) / 5
```

We can tell R to same thing with functions.

```{r}
x <- c(56, 31, 56, 8, 32)
sum(x) / length(x)
```

Although it’s pretty easy to calculate the mean using the `sum()` function, we can do it in an even easier way, since R also provides us with the `mean()` function.

```{r}
x <- c(56, 31, 56, 8, 32)
mean(x, na.rm = TRUE)
```

#### Which one? Median vs Mean

Knowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. 

![](images/clipboard-3927156023.png)

The mean is kind of like the “centre of gravity” of the data set, whereas the median is the “middle value” in the data. What this implies, as far as which one you should use, depends a little on what type of data you’ve got and what you’re trying to achieve. As a rough guide:

-   **Nominal data:** Use the mode.

-   **Ordinal data:** More likely to want to use the median than the mean. The median only makes use of the order information in your data (i.e., which numbers are bigger), but doesn’t depend on the precise numbers involved. That’s exactly the situation that applies when your data are ordinal scale. The mean, on the other hand, makes use of the precise numeric values assigned to the observations, so it’s not really appropriate for ordinal data.

-   **Interval** and **Ratio** scale data: either mean or median is generally acceptable. Which one you pick depends a bit on what you’re trying to achieve. The mean has the advantage that it uses all the information in the data (which is useful when you don’t have a lot of data), but it’s very sensitive to extreme values.

------------------------------------------------------------------------

### Weighted Mean

*An average where some of the values count more than others. Instead of treating every value equally, each value is multiplied by an assigned weight.*

Computed by multiplying each observation by its corresponding weight, summing those products, and dividing by the sum of the weights.

$$
\bar{x}= \frac{w_1 x_1 + w_2 x_2 + \cdots + w_n x_n}       {w_1 + w_2 + \cdots + w_n}= \frac{\sum w_i x_i}{\sum w_i}
$$

where $x_i$ is the i-th observation and $w_i$ is its corresponding weight.

```{r}
x <- c(70, 80, 90) # observations
w <- c(50, 40, 10)  # weights

sum(x * w) / sum(w)
```

R has build-in function for this.

```{r}
weighted.mean(x, w)
```

### Trimmed Mean

*Discards a percentage of extreme values from both ends and calculates the mean*

$$
\bar{x}_{\text{trim}} =\frac{x_{(k+1)} + x_{(k+2)} + \cdots + x_{(n-k)}}{n - 2k}
$$

where $x_{(i)}$ denotes the i-th ordered observation and $k$ is the number of values trimmed from each tail.

For example, consider this strange looking data set:

$$ -100, 2, 3, 4, 5, 6, 7, 8, 9, 10 $$

If you were to observe this in a real life data set, you’d probably suspect that something funny was going on with the −100 value. It’s probably an ***outlier***, a value that doesn’t really belong with the others. You might consider removing it from the data set entirely, and in this particular case I’d probably agree with that course of action. In real life, however, you don’t always get such cut-and-dried examples. For instance, you might get this instead:\
$$ −15,2,3,4,5,6,7,8,9,12 $$

The −15 looks a bit suspicious, but not anywhere near as much as that −100 did. In this case, it’s a little trickier. It *might* be a legitimate observation, it might not.

When faced with a situation where some of the most extreme-valued observations might not be quite trustworthy, the mean is not necessarily a good measure of central tendency. It is highly sensitive to one or two extreme values, and is thus not considered to be a ***robust*** measure. 

One remedy that we’ve seen is to use the median. A more general solution is to use a “trimmed mean”. 

To calculate a trimmed mean, what you do is “discard” the most extreme examples on both ends (i.e., the largest and the smallest), and then take the mean of everything else. The goal is to preserve the best characteristics of the mean and the median: just like a median, you aren’t highly influenced by extreme outliers, but like the mean, you “use” more than one of the observations. 

Generally, we describe a trimmed mean in terms of the percentage of observation on either side that are discarded. So, for instance, a 10% trimmed mean discards the largest 10% of the observations *and* the smallest 10% of the observations, and then takes the mean of the remaining 80% of the observations. Not surprisingly, the 0% trimmed mean is just the regular mean, and the 50% trimmed mean is the median. In that sense, trimmed means provide a whole family of central tendency measures that span the range from the mean to the median.

For our toy example above, we have 10 observations, and so a 10% trimmed mean is calculated by ignoring the largest value (i.e., `12`) and the smallest value (i.e., `-15`) and taking the mean of the remaining values.

First, let’s enter the data.

```{r}
x <- c(-15,2,3,4,5,6,7,8,9,12)
```

Next, let's calculate means and medians:

```{r}
mean(x)
```

```{r}
median(x)
```

That’s a fairly substantial difference, but I’m tempted to think that the mean is being influenced a bit too much by the extreme values at either end of the data set, especially the −15 one. So let’s just try trimming the mean a bit. If I take a 10% trimmed mean, we’ll drop the extreme values on either side, and take the mean of the rest:

```{r}
mean(x, trim = .1)
```

which in this case gives exactly the same answer as the median. Note that, to get a 10% trimmed mean you write `trim = .1`, not `trim = 10`.

------------------------------------------------------------------------

## Skew and Kurtosis

There are two more descriptive statistics that you will sometimes see reported in the psychological literature, known as skew and kurtosis. In practice, neither one is used anywhere near as frequently as the measures of central tendency and variability that we’ve been talking about.

Skew is pretty important, so you do see it mentioned a fair bit; but I’ve actually never seen kurtosis reported in a scientific article to date.

### Skew

Since it’s the more interesting of the two, let’s start by talking about the ***skewness***. Skewness is basically a measure of asymmetry, and the easiest way to explain it is by drawing some pictures. 

Right skew or positively skewed: Tail on the right.

Left skew or negatively skewed: Tail on the left.

```{r}
set.seed(123)

n <- 1000

# Positive skew: Gamma (right tail)
pos <- rgamma(n, shape = 6, scale = 1)

# No skew: Normal
noskew <- rnorm(n, mean = mean(pos), sd = sd(pos))

# Negative skew: reflect the positive skew around its max (left tail)
neg <- max(pos) - pos

df <- bind_rows(
  tibble(x = neg,    skew = "Negative Skew"),
  tibble(x = noskew, skew = "No Skew"),
  tibble(x = pos,    skew = "Positive Skew")
)

ggplot(df, aes(x)) +
  geom_histogram(bins = 20, fill = "cornflowerblue", color = "white") +
  facet_wrap(~ skew, nrow = 1, scales = "free_x") +
  theme_minimal(base_size = 14)
```

### Kurtosis

The final measure that is sometimes referred to, though very rarely in practice, is the ***kurtosis*** of a data set. Put simply, kurtosis is a measure of the “pointiness” of a data set.

```{r}
set.seed(123)
n <- 1000

df <- bind_rows(
  tibble(x = runif(n, -sqrt(3),  sqrt(3)), type = "Platykurtic\n(\"too flat\")"),
  tibble(x = rnorm(n),                      type = "Mesokurtic"),
  tibble(x = rt(n, df = 3) / sd(rt(n, df = 3)), type = "Leptokurtic\n(\"too pointy\")")
) %>%
  mutate(type = factor(type, levels = c(
    "Platykurtic\n(\"too flat\")",
    "Mesokurtic",
    "Leptokurtic\n(\"too pointy\")"
  )))

ggplot(df, aes(x)) +
  geom_histogram(aes(y = after_stat(density)),
                 bins = 22, fill = "cornflowerblue", color = "white") +
  stat_function(fun = dnorm, linewidth = 1) +
  facet_wrap(~ type, nrow = 1) +
  theme_classic(base_size = 14) +
  theme(strip.background = element_blank())
```

------------------------------------------------------------------------

### Moving Average

Adding consecutive observations for a number of periods and dividing the result by the number of periods included in the average.

```{r}
#| message: false
#| warning: false
set.seed(123)

df <- data.frame(
  time = 1:30,
  value = cumsum(rnorm(30))
)

df$ma_3 <- stats::filter(df$value, rep(1/3, 3), sides = 2)


library(ggplot2)

ggplot(df, aes(x = time)) +
  geom_line(aes(y = value), alpha = 0.6) +
  geom_line(aes(y = ma_3), linewidth = 1.2) +
  theme_classic(base_size = 14) +
  labs(
    title = "Moving Average Smooths Short-Term Fluctuations",
    x = "Time",
    y = "Value"
  )

```

------------------------------------------------------------------------

## Dispersion

### Range

*The difference between the largest and smallest data values*

```{r}
max_mass <- max(starwars$mass, na.rm = TRUE)
min_mass <- min(starwars$mass, na.rm = TRUE)

max_mass
min_mass
```

```{r}
cat("[",min_mass, ", ", max_mass, "]")
```

Or simply run the function...

```{r}
range(starwars$mass, na.rm = TRUE)
```

Although the range is the simplest way to quantify the notion of “variability”, it’s one of the worst. If the data set has one or two extremely bad values in it, we’d like our statistics not to be unduly influenced by these cases.

### Variability

The amount of individual observation or set of observations fluctuates about the mean or center of set of data.

Deviation is distance that a value is from the mean.

#### Sample Variance

For each observation, calculate the distance from the mean. Square. Then, sum. Divide by n-1.

$$
s^2 = \frac{\sum (x_i - \bar{x})^2}{n - 1}
$$

Imagine that we have the heights of 5 dogs, measured at their shoulder.

$$
600mm, 470mm, 170mm, 430mm, 300mm
$$

```{r}
x <- c(600, 470, 170, 430, 300)
```

If I was doing this by hand, I'll need to know the mean to use in the formula.

```{r}
xbar <- mean(x)
```

Next, subtract the mean from each dog's height.

```{r}
difference <- x - xbar
difference
```

Square it.

```{r}
squared <- (difference * difference)
squared
```

Sum it up.

```{r}
summed_up <- sum(squared)
summed_up
```

Divided by n - 1.

```{r}
variance <- summed_up / length(x)
variance
```

### Mean Absolute Deviation (MAD)

$$
\mathrm{MAD} = \frac{\sum \lvert x_i - \bar{x} \rvert}{n}
$$

One possibility would be to do everything using low level commands, laboriously following the calculations above. However, that’s pretty tedious.

You’d end up with a series of commands that might look like this:

```{r}
X <- c(600, 470, 170, 430, 300)   # enter the data
```

```{r}
X.bar <- mean( X )       # step 1. the mean of the data
AD <- abs( X - X.bar )   # step 2. the absolute deviations from the mean
MAD <- mean( AD )        # step 3. the mean absolute deviations
print( MAD )             # print the results
```

Each of those commands is pretty simple, but there’s just too many of them. And because I find that to be too much typing, the `lsr` package has a very simple function called `aad()` that does the calculations for you. If we apply the `aad()` function to our data, we get this:

```{r}
lsr::aad(X)
```

No suprises there.

### Standard Deviation

Simply take the square root of the variance.

```{r}
SD <- sqrt(variance)
SD
```

For population SD, use N. For sample SD use n-1.

#### Why use square differences?

If we just add up the differences from the mean the negatives cancel the positives:

$$
\frac{4 + 4 - 4 - 4}{4} = 0
$$

```{r}
#| echo: false
df4 <- data.frame(
  x = c(1, 2, 3, 4),
  y = c(4, 4, -4, -4),
  label = c("+4", "+4", "-4", "-4")
)

ggplot(df4, aes(x = x, y = y)) +
  geom_hline(yintercept = 0, color = "magenta", linewidth = 1) +
  geom_segment(aes(xend = x, y = 0, yend = y),
               linewidth = 1, color = "gray40") +
  geom_point(size = 3, color = "orange") +
  geom_text(aes(label = label),
            vjust = ifelse(df4$y > 0, -1, 1.5),
            size = 5) +
  ylim(-6, 6) +
  theme_void()
```

What about absolute values? That's MAD.

$$
\frac{|4| + |4| + |-4| + |-4|}{4}= \frac{4 + 4 + 4 + 4}{4}= 4
$$

Not to bad, but.... it does not always work out. Consider this:

$$
\frac{|7| + |1| + |-6| + |-2|}{4}= \frac{7 + 1 + 6 + 2}{4}= 4
$$

Clearly more spread than the first one, but the same results.

```{r}
df7 <- data.frame(
  x = c(1, 2, 3, 4),
  y = c(7, 1, -6, -2),
  label = c("+7", "+1", "-6", "-2")
)

ggplot(df7, aes(x = x, y = y)) +
  geom_hline(yintercept = 0, color = "magenta", linewidth = 1) +
  geom_segment(aes(xend = x, y = 0, yend = y),
               linewidth = 1, color = "gray40") +
  geom_point(size = 3, color = "orange") +
  geom_text(
    aes(label = label),
    vjust = ifelse(df7$y > 0, -0.8, 1.3),
    size = 5
  ) +
  scale_y_continuous(
    limits = c(min(df7$y) - 2, max(df7$y) + 2)
  ) +
  theme_void()
```

So lets try to square them and then take the square root at the end.

$$
\sqrt{\frac{4^2 + 4^2 + (-4)^2 + (-4)^2}{4}}= \sqrt{\frac{64}{4}}= 4
$$

$$
\sqrt{\frac{7^2 + 1^2 + (-6)^2 + (-2)^2}{4}}= \sqrt{\frac{90}{4}}= 4.74
$$

Success! The standard deviation is bigger when the data is more spread out. Hence, why we use this approach.

## Empirical Rule

The variability of a set of measurements for a **bell-shaped distribution**. Most values cluster around the mean.

-   About **68%** of the data fall within **1** standard deviation of the mean

<!-- -->

-   About **95%** of the data fall within **2** standard deviations of the mean

<!-- -->

-   About **99.7%** of the data fall within **3** standard deviations of the mean

```{r}
set.seed(123)
x <- rnorm(10000)
df <- data.frame(x)

mu <- mean(df$x)
sdx <- sd(df$x)

ggplot(df, aes(x)) +
  stat_function(fun = dnorm, args = list(mean = mu, sd = sdx)) +

  # 68%
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sdx),
    xlim = c(mu - sdx, mu + sdx),
    geom = "area",
    alpha = 0.4
  ) +

  # 95%
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sdx),
    xlim = c(mu - 2 * sdx, mu + 2 * sdx),
    geom = "area",
    alpha = 0.25
  ) +

  # 99.7%
  stat_function(
    fun = dnorm,
    args = list(mean = mu, sd = sdx),
    xlim = c(mu - 3 * sdx, mu + 3 * sdx),
    geom = "area",
    alpha = 0.15
  ) +

  theme_classic(base_size = 14) +
  labs(
    title = "Empirical Rule (68–95–99.7)",
    x = "Mean",
    y = "Density") + 
  scale_x_continuous(
  breaks = mu + (-3:3) * sdx,
  labels = c("-3 SD", "-2 SD", "-1 SD", "0", "+1 SD", "+2 SD", "+3 SD")
)
```

### Chebyshev's Theorem

General rule that describes the variability of any set of data regardless of the shape of its distribution.

$$
1 - \frac{1}{k^2}, \quad \text{for } k > 1
$$

## Coefficient of Variation

Compares the variation in data sets.

$$
\mathrm{CV} = \left( \frac{s}{\bar{x}} \cdot 100 \right)\%
$$

## Additional Functions

```{r}
summary(starwars$height)
```

```{r}
psych::describe(starwars$height)
```

Acknowledgements: learningstatisticswithR
